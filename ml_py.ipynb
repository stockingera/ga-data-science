{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Workflows with Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python with scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Notebook provides an example of how to automate **Machine Learning (ML) workflows** with [scikit-learn](http://scikit-learn.org/) and sheds some light on: \n",
    "\n",
    "> 1. [Evaluate the Performance of ML algorithms in Python using resampling](#Evaluate-the-Performance-of-ML-algorithms-in-Python-using-resampling)\n",
    "1. [Metrics to evaluate performance](#Metrics-To-Evaluate-Machine-Learning-Algorithms-in-Python)\n",
    "    1. [Classification Metrics](#Classification-Metrics)\n",
    "    2. [Regression Metrics](#Regression-Metrics)\n",
    "1. [Test classification ML algorithms](#Test-classification-ML-algorithms)\n",
    "1. [Test regression ML algorithms](#Test-regression-ML-algorithms)\n",
    "1. [Compare ML algorithms](#Compare-Machine-Learning-Algorithms)\n",
    "1. [Save and Load Models](#Save-and-Load-Models)\n",
    "1. [Automate the process using the **Pipeline** and **FeatureUnion** utilities to avoid data leakage](#Automate-the-process-using-the-Pipeline-and-FeatureUnion-utilities-to-avoid-data-leakage).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the Performance of ML algorithms in Python using resampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How well will your ML algorithms perform?\n",
    "\n",
    "- Make predictions for new data for known answers\n",
    "- Resampling methods estimates performance on new data.\n",
    "\n",
    "> It is important to evaluate ML algorithms using data that is different than the data used to train ML algorithm, to avoid **overfitting**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Four techniques will be demonstrated:\n",
    "\n",
    "> 1. Train and Test Sets\n",
    "1. K-fold Cross Validation\n",
    "1. Leave One Out Cross Validation\n",
    "1. Repeated Random Test-Train Splits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Train and Test Sets\n",
    "\n",
    "It is common to split the data set into test set and training set. This allows us to compare predicted results vs actual results in the test split. The training split is used to train the algorithm. The split is usually 70/30 for training/testing, respectively.\n",
    "\n",
    "- **Pros**: works on very large data sets and is fast\n",
    "- **Cons**: high variance\n",
    "\n",
    "Example below splits Pima Indians dataset 70/30. This is testing the accuracy of a Logistic Regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 76.190%\n"
     ]
    }
   ],
   "source": [
    "# Evaluate using a train and a test set\n",
    "import pandas\n",
    "from sklearn import cross_validation\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "url = \"pima.data\"\n",
    "names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
    "dataframe = pandas.read_csv(url, names=names)\n",
    "array = dataframe.values\n",
    "X = array[:,0:8]\n",
    "Y = array[:,8]\n",
    "test_size = 0.30\n",
    "seed = 7\n",
    "X_train, X_test, Y_train, Y_test = cross_validation.train_test_split(X, Y, test_size=test_size, random_state=seed)\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, Y_train)\n",
    "result = model.score(X_test, Y_test)\n",
    "print(\"Accuracy: %.3f%%\") % (result*100.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. K-fold Cross Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like train/test split method, K-fold splits data train/test splits but in several **folds**, known as k-parts. It is common to use values of 3, 5 or 10 for folds. The mean and standard deviation for the results of the folds need to be calculate to obtain an end result.\n",
    "\n",
    "- **Pros**: less variance then the train/test method.\n",
    "- **Cons**: may be difficult to determine optimal number of folds for very large datasets\n",
    "\n",
    "Example below uses 10 for number of folds. It also uses Gaussian distribution of performance, mean and standard deviation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 76.951% (4.841%)\n"
     ]
    }
   ],
   "source": [
    "# Evaluate using Cross Validation\n",
    "import pandas\n",
    "from sklearn import cross_validation\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "url = \"pima.data\"\n",
    "names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
    "dataframe = pandas.read_csv(url, names=names)\n",
    "array = dataframe.values\n",
    "X = array[:,0:8]\n",
    "Y = array[:,8]\n",
    "num_folds = 10\n",
    "num_instances = len(X)\n",
    "seed = 7\n",
    "kfold = cross_validation.KFold(n=num_instances, n_folds=num_folds, random_state=seed)\n",
    "model = LogisticRegression()\n",
    "results = cross_validation.cross_val_score(model, X, Y, cv=kfold)\n",
    "print(\"Accuracy: %.3f%% (%.3f%%)\") % (results.mean()*100.0, results.std()*100.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Leave One Out Cross Validation\n",
    "\n",
    "Leave-one-out cross validation is when the fold is 1 (k is set to the number of observations in your dataset). \n",
    "\n",
    "Performance results are summarized. Can be more computationally intensive than K-fold.\n",
    "\n",
    "- **Pros**: offers large number of performance results than can be summarized\n",
    "- **Cons**: computationally more expensive than k-fold cross validation\n",
    "\n",
    "In the example below we use leave-one-out cross validation, the results have more variance on the computed data than k-fold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 76.823% (42.196%)\n"
     ]
    }
   ],
   "source": [
    "# Evaluate using Leave One Out Cross Validation\n",
    "import pandas\n",
    "from sklearn import cross_validation\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "url = \"pima.data\"\n",
    "names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
    "dataframe = pandas.read_csv(url, names=names)\n",
    "array = dataframe.values\n",
    "X = array[:,0:8]\n",
    "Y = array[:,8]\n",
    "num_folds = 10\n",
    "num_instances = len(X)\n",
    "loocv = cross_validation.LeaveOneOut(n=num_instances)\n",
    "model = LogisticRegression()\n",
    "results = cross_validation.cross_val_score(model, X, Y, cv=loocv)\n",
    "print(\"Accuracy: %.3f%% (%.3f%%)\") % (results.mean()*100.0, results.std()*100.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  4. Repeated Random Test-Train Splits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, do a train/test split as above, but repeat, similar to k-fold.\n",
    "\n",
    "- **Pros**: speed of train/test split and reduction in variance of k-fold cross validation.\n",
    "- **Cons**: redundancy is likely as the same data may be used to train/test.\n",
    "\n",
    "The example below splits the data into a 80%/30% train/test split and repeats the process 10 times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 76.970% (1.366%)\n"
     ]
    }
   ],
   "source": [
    "# Evaluate using Shuffle Split Cross Validation\n",
    "import pandas\n",
    "from sklearn import cross_validation\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "url = \"pima.data\"\n",
    "names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
    "dataframe = pandas.read_csv(url, names=names)\n",
    "array = dataframe.values\n",
    "X = array[:,0:8]\n",
    "Y = array[:,8]\n",
    "num_samples = 10\n",
    "test_size = 0.3\n",
    "num_instances = len(X)\n",
    "seed = 7\n",
    "kfold = cross_validation.ShuffleSplit(n=num_instances, n_iter=num_samples, test_size=test_size, random_state=seed)\n",
    "model = LogisticRegression()\n",
    "results = cross_validation.cross_val_score(model, X, Y, cv=kfold)\n",
    "print(\"Accuracy: %.3f%% (%.3f%%)\") % (results.mean()*100.0, results.std()*100.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "
\n",
    "
\n",
    "What Techniques to Use When\n",
    "
\n",
    "\n",
    "- K-fold cross validation is the most commonly used method to test for unknown data with k set to 3, 5, or 10.\n",
    "- Using a train/test split is good for speed when using a slow algorithm and produces performance estimates with lower bias when using large datasets.\n",
    "- Leave-one-out cross validation and repeated random splits can be useful intermediates when trying to balance variance in the estimated performance, model training speed and dataset size.\n",
    "- Find something that is fast and performant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics To Evaluate Machine Learning Algorithms in Python\n",
    "\n",
    "In these examples, **Logistic Regression for classification** and **Linear Regression for the regression** are used, using 10-fold cross-validation for each metric. Further comparisons could be achieved with the other options described in the *Evaluate the Performance of ML algorithms in Python using resampling* section.\n",
    "\n",
    "Datasets used: for **classification metrics**, the Pima Indians onset of diabetes dataset is used. For **regression metrics**, the Boston House Price dataset is used as demonstration. Both are from the [UCI Machine Learning repository](https://archive.ics.uci.edu/ml/index.html).\n",
    "\n",
    "More information can be found about finding the scikit-learn's [Model evaluation: quantifying the quality of predictions](http://scikit-learn.org/stable/modules/model_evaluation.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification Metrics\n",
    "\n",
    "Classification algorithms are probably the most common type of ML algorithm, therefore the number of performance metrics is large. Below are the most common:\n",
    "\n",
    "> 1. [Classification Accuracy](#[Performance-Metric]-1.-Classification-accuracy).\n",
    "1. [Logarithmic Loss](#[Performance-Metric]-2.-Logarithmic-Loss).\n",
    "1. [Area Under ROC Curve](#[Performance-Metric]-3.-Area-Under-ROC-Curve).\n",
    "\n",
    "Also, it is common to use report summaries using:\n",
    "\n",
    "> 1. [Confusion Matrix](#[Classification-Metrics-Results]-4.-Confusion-Matrix).\n",
    "1. [Classification Report](#[Classification-Metrics-Results]-5.-Classification-Report)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Performance Metric] 1. Classification accuracy \n",
    "\n",
    "Classification accuracy = correct predictions / all predictions\n",
    "\n",
    "Most comon metric, but most misused. Ideal when there are an equal number of observations in each class and that all predictions and prediction errors have same weight. Both of these aren't likely, most of the time.\n",
    "\n",
    "Below is an example of calculating classification accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.770 (0.048)\n"
     ]
    }
   ],
   "source": [
    "# Cross Validation Classification Accuracy\n",
    "import pandas\n",
    "from sklearn import cross_validation\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "url = \"pima.data\"\n",
    "names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
    "dataframe = pandas.read_csv(url, names=names)\n",
    "array = dataframe.values\n",
    "X = array[:,0:8]\n",
    "Y = array[:,8]\n",
    "num_folds = 10\n",
    "num_instances = len(X)\n",
    "seed = 7\n",
    "kfold = cross_validation.KFold(n=num_instances, n_folds=num_folds, random_state=seed)\n",
    "model = LogisticRegression()\n",
    "scoring = 'accuracy'\n",
    "results = cross_validation.cross_val_score(model, X, Y, cv=kfold, scoring=scoring)\n",
    "print(\"Accuracy: %.3f (%.3f)\") % (results.mean(), results.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Performance Metric] 2. Logarithmic Loss\n",
    "\n",
    "Evaluates scalar probability between 0 and 1 of the probabilities of membership to a given class.\n",
    "Correct or incorrect preductions are scored, related to the confidence of the prediction.\n",
    "\n",
    "The example below calculates logloss for Logistical regression (smaller is better).\n",
    "\n",
    "> *cross_val_score()* function inverts score, to be able to order and compare metrics in ascending or descending order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logloss: -0.493 (0.047)\n"
     ]
    }
   ],
   "source": [
    "# Cross Validation Classification LogLoss\n",
    "import pandas\n",
    "from sklearn import cross_validation\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "url = \"pima.data\"\n",
    "names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
    "dataframe = pandas.read_csv(url, names=names)\n",
    "array = dataframe.values\n",
    "X = array[:,0:8]\n",
    "Y = array[:,8]\n",
    "num_folds = 10\n",
    "num_instances = len(X)\n",
    "seed = 7\n",
    "kfold = cross_validation.KFold(n=num_instances, n_folds=num_folds, random_state=seed)\n",
    "model = LogisticRegression()\n",
    "scoring = 'log_loss'\n",
    "results = cross_validation.cross_val_score(model, X, Y, cv=kfold, scoring=scoring)\n",
    "print(\"Logloss: %.3f (%.3f)\") % (results.mean(), results.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Performance Metric] 3. Area Under ROC Curve\n",
    "\n",
    "The Area Under ROC Curve [AUC](https://www.kaggle.com/wiki/AreaUnderCurve) is a common performance metric for binary classification problems. The AUC represents a model’s ability to discriminate between positive and negative classes. An area of 1.0 represents a model that made all predictions perfectly. An area of 0.5 represents a model as good as random.\n",
    "\n",
    "Receiver Operating Characteristic [ROC](https://en.wikipedia.org/wiki/Receiver_operating_characteristic) plots the true positive rate against the false positive rate.\n",
    "\n",
    "The example below provides a demonstration of calculating AUC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC: 0.824 (0.041)\n"
     ]
    }
   ],
   "source": [
    "# Cross Validation Classification ROC AUC\n",
    "import pandas\n",
    "from sklearn import cross_validation\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "url = \"pima.data\"\n",
    "names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
    "dataframe = pandas.read_csv(url, names=names)\n",
    "array = dataframe.values\n",
    "X = array[:,0:8]\n",
    "Y = array[:,8]\n",
    "num_folds = 10\n",
    "num_instances = len(X)\n",
    "seed = 7\n",
    "kfold = cross_validation.KFold(n=num_instances, n_folds=num_folds, random_state=seed)\n",
    "model = LogisticRegression()\n",
    "scoring = 'roc_auc'\n",
    "results = cross_validation.cross_val_score(model, X, Y, cv=kfold, scoring=scoring)\n",
    "print(\"AUC: %.3f (%.3f)\") % (results.mean(), results.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Classification Metrics Results] 4. Confusion Matrix\n",
    "\n",
    "The [confusion matrix](https://en.wikipedia.org/wiki/Confusion_matrix) is used to present the accuracy of a model with two or more classes.\n",
    "\n",
    "The table presents predictions on the x-axis and accuracy outcomes on the y-axis. The cells represent the number or predictions made by the machine learning algorithm.\n",
    "\n",
    "Below is an example of calculating a confusion matrix for a set of predictions by a model on a test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[141  21]\n",
      " [ 41  51]]\n"
     ]
    }
   ],
   "source": [
    "# Cross Validation Classification Confusion Matrix\n",
    "import pandas\n",
    "from sklearn import cross_validation\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix\n",
    "url = \"pima.data\"\n",
    "names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
    "dataframe = pandas.read_csv(url, names=names)\n",
    "array = dataframe.values\n",
    "X = array[:,0:8]\n",
    "Y = array[:,8]\n",
    "test_size = 0.33\n",
    "seed = 7\n",
    "X_train, X_test, Y_train, Y_test = cross_validation.train_test_split(X, Y, test_size=test_size, random_state=seed)\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, Y_train)\n",
    "predicted = model.predict(X_test)\n",
    "matrix = confusion_matrix(Y_test, predicted)\n",
    "print(matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Classification Metrics Results] 5. Classification Report\n",
    "\n",
    "Scikit-learn provides an summarized report when working on classification problems with a number of performance measures.\n",
    "\n",
    "The **classification_report()** function displays the precision, recall, f1-score and support for each class.\n",
    "\n",
    "The example below demonstrates the report on the binary classification problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.77      0.87      0.82       162\n",
      "        1.0       0.71      0.55      0.62        92\n",
      "\n",
      "avg / total       0.75      0.76      0.75       254\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Cross Validation Classification Report\n",
    "import pandas\n",
    "from sklearn import cross_validation\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "url = \"pima.data\"\n",
    "names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
    "dataframe = pandas.read_csv(url, names=names)\n",
    "array = dataframe.values\n",
    "X = array[:,0:8]\n",
    "Y = array[:,8]\n",
    "test_size = 0.33\n",
    "seed = 7\n",
    "X_train, X_test, Y_train, Y_test = cross_validation.train_test_split(X, Y, test_size=test_size, random_state=seed)\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, Y_train)\n",
    "predicted = model.predict(X_test)\n",
    "report = classification_report(Y_test, predicted)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression Metrics\n",
    "\n",
    "Three of the most common peformance metrics for regression are:\n",
    "\n",
    "> 1. [Mean Absolute Error](#1.-[Performance-Metric]-Mean-Absolute-Error).\n",
    "1. [Mean Squared Error](#2.-[Performance-Metric]-Mean-Squared-Error).\n",
    "1. [R^2](#[Performance-Metric]-3.-R^2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. [Performance Metric] Mean Absolute Error\n",
    "\n",
    "The [Mean Absolute Error (or MAE)](https://www.google.com/webhp?sourceid=chrome-instant&ion=1&espv=2&ie=UTF-8#q=Mean+Absolute+Error+(or+MAE)+wikipedia) is the sum of the absolute differences between predictions and actual values. It answers the question: how wrong were the predictions? The performance metric only tells us if it deviated from actual results, but not if the preductions were over or under the actual results.\n",
    "\n",
    "Below is an example that calculates MAE on the UCI Boston house price dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE: -4.005 (2.084)\n"
     ]
    }
   ],
   "source": [
    "# Cross Validation Regression MAE\n",
    "import pandas\n",
    "from sklearn import cross_validation\n",
    "from sklearn.linear_model import LinearRegression\n",
    "url = \"housing.data\"\n",
    "names = ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT', 'MEDV']\n",
    "dataframe = pandas.read_csv(url, delim_whitespace=True, names=names)\n",
    "array = dataframe.values\n",
    "X = array[:,0:13]\n",
    "Y = array[:,13]\n",
    "num_folds = 10\n",
    "num_instances = len(X)\n",
    "seed = 7\n",
    "kfold = cross_validation.KFold(n=num_instances, n_folds=num_folds, random_state=seed)\n",
    "model = LinearRegression()\n",
    "scoring = 'mean_absolute_error'\n",
    "results = cross_validation.cross_val_score(model, X, Y, cv=kfold, scoring=scoring)\n",
    "print(\"MAE: %.3f (%.3f)\") % (results.mean(), results.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. [Performance Metric] Mean Squared Error\n",
    "\n",
    "Like MAE, the [Mean Squared Error (or MSE)](https://en.wikipedia.org/wiki/Mean_squared_error) tells us by how much the predicted results varied from the actual results.\n",
    "\n",
    "Calculating the square root of the MSE allows us to compare performance values with original output values, which is helpful when presenting results in tables and graphs. This is called the Root Mean Squared Error (or RMSE).\n",
    "\n",
    "Below is an example of calculating mean squared error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: -34.705 (45.574)\n"
     ]
    }
   ],
   "source": [
    "# Cross Validation Regression MSE\n",
    "import pandas\n",
    "from sklearn import cross_validation\n",
    "from sklearn.linear_model import LinearRegression\n",
    "url = \"housing.data\"\n",
    "names = ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT', 'MEDV']\n",
    "dataframe = pandas.read_csv(url, delim_whitespace=True, names=names)\n",
    "array = dataframe.values\n",
    "X = array[:,0:13]\n",
    "Y = array[:,13]\n",
    "num_folds = 10\n",
    "num_instances = len(X)\n",
    "seed = 7\n",
    "kfold = cross_validation.KFold(n=num_instances, n_folds=num_folds, random_state=seed)\n",
    "model = LinearRegression()\n",
    "scoring = 'mean_squared_error'\n",
    "results = cross_validation.cross_val_score(model, X, Y, cv=kfold, scoring=scoring)\n",
    "print(\"MSE: %.3f (%.3f)\") % (results.mean(), results.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Performance Metric] 3. R^2\n",
    "\n",
    "The [R^2 (or R Squared)](https://en.wikipedia.org/wiki/Coefficient_of_determination) performance metric provides results on the goodness of fit of a set of predictions to the actual values, also called the *coefficient of determination*.\n",
    "\n",
    "0 is for no-fit and 1 is a perfect fit.\n",
    "\n",
    "Below is an example for R^2 using the UCI housing dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R^2: 0.203 (0.595)\n"
     ]
    }
   ],
   "source": [
    "# Cross Validation Regression R^2\n",
    "import pandas\n",
    "from sklearn import cross_validation\n",
    "from sklearn.linear_model import LinearRegression\n",
    "url = \"housing.data\"\n",
    "names = ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT', 'MEDV']\n",
    "dataframe = pandas.read_csv(url, delim_whitespace=True, names=names)\n",
    "array = dataframe.values\n",
    "X = array[:,0:13]\n",
    "Y = array[:,13]\n",
    "num_folds = 10\n",
    "num_instances = len(X)\n",
    "seed = 7\n",
    "kfold = cross_validation.KFold(n=num_instances, n_folds=num_folds, random_state=seed)\n",
    "model = LinearRegression()\n",
    "scoring = 'r2'\n",
    "results = cross_validation.cross_val_score(model, X, Y, cv=kfold, scoring=scoring)\n",
    "print(\"R^2: %.3f (%.3f)\") % (results.mean(), results.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test classification ML algorithms\n",
    "\n",
    "It's impossible to know what classification algorithm(s) to use from the get go. Therefore, it's necessary to have a test harness to filter through potential options.\n",
    "\n",
    "Classification algorithms can be summarized as:\n",
    "\n",
    "1. Two [Linear ML Algorithms](#Linear-ML-Algorithms):\n",
    "\n",
    "    1. [Logistic Regression](#Logistic-Regression)\n",
    "    1. [Linear Discriminant Analysis](#Linear-Discriminant-Analysis)\n",
    "\n",
    "1. Four [Nonlinear ML Algorithms](#Nonlinear-Machine-Learning-Algorithms):\n",
    "\n",
    "    1. [K-Nearest Neighbors](#K-Nearest-Neighbors)\n",
    "    1. [Naive Bayes](#Naive-Bayes)\n",
    "    1. [Classification and Regression Trees](#Classification-and-Regression-Trees)\n",
    "    1. [Support Vector Machines](#Support-Vector-Machines)\n",
    "\n",
    "10-fold cross validation is used to demonstrate how to test each ML algorithm and mean accuracy measures are used to illustrate performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear ML Algorithms\n",
    "\n",
    "#### Logistic Regression\n",
    "\n",
    "- Assumes a Gaussian distribution for the numeric input variables and can model binary classification problems. \n",
    "- This example uses the [LogisticRegression](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) class. \n",
    "- Returns the *mean estimated accuracy*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.76951469583\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression Classification\n",
    "import pandas\n",
    "from sklearn import cross_validation\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "url = \"pima.data\"\n",
    "names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
    "dataframe = pandas.read_csv(url, names=names)\n",
    "array = dataframe.values\n",
    "X = array[:,0:8]\n",
    "Y = array[:,8]\n",
    "num_folds = 10\n",
    "num_instances = len(X)\n",
    "seed = 7\n",
    "kfold = cross_validation.KFold(n=num_instances, n_folds=num_folds, random_state=seed)\n",
    "model = LogisticRegression()\n",
    "results = cross_validation.cross_val_score(model, X, Y, cv=kfold)\n",
    "print(results.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear Discriminant Analysis\n",
    "\n",
    "- Linear Discriminant Analysis (LDA) is a statistical technique for binary and multi-class classification. \n",
    "- Assumes a Gaussian distribution for the numerical input variables.\n",
    "- This example uses the [LinearDiscriminantAnalysis](http://scikit-learn.org/stable/modules/generated/sklearn.discriminant_analysis.LinearDiscriminantAnalysis.html) class.\n",
    "- Returns the *mean estimated accuracy*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.773462064252\n"
     ]
    }
   ],
   "source": [
    "# LDA Classification\n",
    "import pandas\n",
    "from sklearn import cross_validation\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "url = \"pima.data\"\n",
    "names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
    "dataframe = pandas.read_csv(url, names=names)\n",
    "array = dataframe.values\n",
    "X = array[:,0:8]\n",
    "Y = array[:,8]\n",
    "num_folds = 10\n",
    "num_instances = len(X)\n",
    "seed = 7\n",
    "kfold = cross_validation.KFold(n=num_instances, n_folds=num_folds, random_state=seed)\n",
    "model = LinearDiscriminantAnalysis()\n",
    "results = cross_validation.cross_val_score(model, X, Y, cv=kfold)\n",
    "print(results.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nonlinear Machine Learning Algorithms\n",
    "\n",
    "#### K-Nearest Neighbors\n",
    "\n",
    "- K-Nearest Neighbors (KNN) finds the most similar instances in the training data and uses the mean distance of the neighbors to calculate a prediction. \n",
    "- This example uses [KNeighborsClassifier](http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html) class.\n",
    "- Returns the *mean estimated accuracy*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.726555023923\n"
     ]
    }
   ],
   "source": [
    "# KNN Classification\n",
    "import pandas\n",
    "from sklearn import cross_validation\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "url = \"pima.data\"\n",
    "names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
    "dataframe = pandas.read_csv(url, names=names)\n",
    "array = dataframe.values\n",
    "X = array[:,0:8]\n",
    "Y = array[:,8]\n",
    "num_folds = 10\n",
    "num_instances = len(X)\n",
    "random_state = 7\n",
    "kfold = cross_validation.KFold(n=num_instances, n_folds=num_folds, random_state=random_state)\n",
    "model = KNeighborsClassifier()\n",
    "results = cross_validation.cross_val_score(model, X, Y, cv=kfold)\n",
    "print(results.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Naive Bayes\n",
    "\n",
    "- Naive Bayes classifies instances based on the assumption that class features are independent of each other.\n",
    "- A Gaussian distribution is assumed to estimate the probabilities for input variables using the Gaussian Probability Density Function.\n",
    "- This example uses the [GaussianNB](http://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html) class.\n",
    "- Returns the *mean estimated accuracy*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.75517771702\n"
     ]
    }
   ],
   "source": [
    "# Gaussian Naive Bayes Classification\n",
    "import pandas\n",
    "from sklearn import cross_validation\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "url = \"pima.data\"\n",
    "names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
    "dataframe = pandas.read_csv(url, names=names)\n",
    "array = dataframe.values\n",
    "X = array[:,0:8]\n",
    "Y = array[:,8]\n",
    "num_folds = 10\n",
    "num_instances = len(X)\n",
    "seed = 7\n",
    "kfold = cross_validation.KFold(n=num_instances, n_folds=num_folds, random_state=seed)\n",
    "model = GaussianNB()\n",
    "results = cross_validation.cross_val_score(model, X, Y, cv=kfold)\n",
    "print(results.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classification and Regression Trees\n",
    "\n",
    "- Classification and Regression Trees (CART or decision trees) build a binary tree from the training data.\n",
    "- Split points are chosen after evaluating attributes and values of each attribute.\n",
    "- This example uses [DecisionTreeClassifier](http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html) class.\n",
    "- Returns the *mean estimated accuracy*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.696548188653\n"
     ]
    }
   ],
   "source": [
    "# CART Classification\n",
    "import pandas\n",
    "from sklearn import cross_validation\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "url = \"pima.data\"\n",
    "names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
    "dataframe = pandas.read_csv(url, names=names)\n",
    "array = dataframe.values\n",
    "X = array[:,0:8]\n",
    "Y = array[:,8]\n",
    "num_folds = 10\n",
    "num_instances = len(X)\n",
    "seed = 7\n",
    "kfold = cross_validation.KFold(n=num_instances, n_folds=num_folds, random_state=seed)\n",
    "model = DecisionTreeClassifier()\n",
    "results = cross_validation.cross_val_score(model, X, Y, cv=kfold)\n",
    "print(results.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Support Vector Machines\n",
    "\n",
    "- Support Vector Machines (or SVM) calculates a line that best separates two classes. Data instances closest to the line are called support vectors. \n",
    "- SVM can support multiple classes.\n",
    "- Kernel parameter allows for different kernel functions. Radial Basis Function is used by default.\n",
    "- This examples uses the [SVC](http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html) class.\n",
    "- Returns the *mean estimated accuracy*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.651025290499\n"
     ]
    }
   ],
   "source": [
    "# SVM Classification\n",
    "import pandas\n",
    "from sklearn import cross_validation\n",
    "from sklearn.svm import SVC\n",
    "url = \"pima.data\"\n",
    "names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
    "dataframe = pandas.read_csv(url, names=names)\n",
    "array = dataframe.values\n",
    "X = array[:,0:8]\n",
    "Y = array[:,8]\n",
    "num_folds = 10\n",
    "num_instances = len(X)\n",
    "seed = 7\n",
    "kfold = cross_validation.KFold(n=num_instances, n_folds=num_folds, random_state=seed)\n",
    "model = SVC()\n",
    "results = cross_validation.cross_val_score(model, X, Y, cv=kfold)\n",
    "print(results.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary of Classification Algorithm Results with Mean Estimated Accuracy\n",
    "\n",
    "- Logistic regression: 0.76951469583\n",
    "- **LDA: 0.773462064252**\n",
    "- KNN: 0.726555023923\n",
    "- Naive Bayes: 0.75517771702\n",
    "- CART: 0.696548188653\n",
    "- SVM: 0.651025290499"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test regression ML algorithms\n",
    "\n",
    "Like classification problems, it's impossible to know what regression algorithm(s) to use from the get go. Therefore, it's necessary to have a test harness to filter through potential options.\n",
    "\n",
    "The algorithms used will be:\n",
    "\n",
    "1. Four [Linear Machine Learning Algorithms](#Linear-Machine-Learning-Algorithms):\n",
    "    1. [Linear Regression](#Linear-Machine-Learning-Algorithms)\n",
    "    1. [Ridge Regression](#Ridge-Regression)\n",
    "    1. [LASSO Linear Regression](#LASSO-Regression)\n",
    "    1. [Elastic Net Regression](#ElasticNet-Regression)\n",
    "\n",
    "1. Three [Nonlinear Machine Learning Algorithms](#Nonlinear-Machine-Learning-Algorithms):\n",
    "    1. [K-Nearest Neighbors](#K-Nearest-Neighbors-[ii])\n",
    "    1. [Classification and Regression Trees](#Classification-and-Regression-Trees)\n",
    "    1. [Support Vector Machines](#Support-Vector-Machines)\n",
    "\n",
    "The Boston House Price dataset will be used since all attributes are numeric. 10-fold cross validation is used to demonstrate how to test each ML algorithm and mean accuracy measures are used to illustrate performance.\n",
    "\n",
    "> Mean squared error values are negative. The *cross_val_score()* function used requires algorithm metrics to be sorted in ascending order (larger value is better)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Machine Learning Algorithms\n",
    "\n",
    "#### Linear Regression\n",
    "\n",
    "- Assumes that the input variables have a Gaussian distribution.\n",
    "- Assumed that input variables are relevant to the output variable and that they are not highly correlated with each other.\n",
    "- Example uses the [LinearRegression](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html) class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-34.7052559445\n"
     ]
    }
   ],
   "source": [
    "# Linear Regression\n",
    "import pandas\n",
    "from sklearn import cross_validation\n",
    "from sklearn.linear_model import LinearRegression\n",
    "url = \"housing.data\"\n",
    "names = ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT', 'MEDV']\n",
    "dataframe = pandas.read_csv(url, delim_whitespace=True, names=names)\n",
    "array = dataframe.values\n",
    "X = array[:,0:13]\n",
    "Y = array[:,13]\n",
    "num_folds = 10\n",
    "num_instances = len(X)\n",
    "seed = 7\n",
    "kfold = cross_validation.KFold(n=num_instances, n_folds=num_folds, random_state=seed)\n",
    "model = LinearRegression()\n",
    "scoring = 'mean_squared_error'\n",
    "results = cross_validation.cross_val_score(model, X, Y, cv=kfold, scoring=scoring)\n",
    "print(results.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ridge Regression\n",
    "\n",
    "- Ridge regression is an extension of linear regression\n",
    "- Loss function is adjusted by imposing a penalty on the size of the coefficients (also called the 12 -norm)\n",
    "- The ridge coefficients minimize a penalized sum squared coefficient values \n",
    "- Example uses the [Ridge](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html) class.\n",
    "- Result is *mean squared error*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-34.0782462093\n"
     ]
    }
   ],
   "source": [
    "# Ridge Regression\n",
    "import pandas\n",
    "from sklearn import cross_validation\n",
    "from sklearn.linear_model import Ridge\n",
    "url = \"housing.data\"\n",
    "names = ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT', 'MEDV']\n",
    "dataframe = pandas.read_csv(url, delim_whitespace=True, names=names)\n",
    "array = dataframe.values\n",
    "X = array[:,0:13]\n",
    "Y = array[:,13]\n",
    "num_folds = 10\n",
    "num_instances = len(X)\n",
    "seed = 7\n",
    "kfold = cross_validation.KFold(n=num_instances, n_folds=num_folds, random_state=seed)\n",
    "model = Ridge()\n",
    "scoring = 'mean_squared_error'\n",
    "results = cross_validation.cross_val_score(model, X, Y, cv=kfold, scoring=scoring)\n",
    "print(results.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LASSO Regression\n",
    "\n",
    "- The Least Absolute Shrinkage and Selection Operator (LASSO) is an extension of linear regression\n",
    "- Loss function is adjusted by calculating the sum absolute coefficient values (also called the 11 -norm).\n",
    "- Example uses the [Lasso](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html) class.\n",
    "- Result is *mean squared error*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-34.4640845883\n"
     ]
    }
   ],
   "source": [
    "# Lasso Regression\n",
    "import pandas\n",
    "from sklearn import cross_validation\n",
    "from sklearn.linear_model import Lasso\n",
    "url = \"housing.data\"\n",
    "names = ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT', 'MEDV']\n",
    "dataframe = pandas.read_csv(url, delim_whitespace=True, names=names)\n",
    "array = dataframe.values\n",
    "X = array[:,0:13]\n",
    "Y = array[:,13]\n",
    "num_folds = 10\n",
    "num_instances = len(X)\n",
    "seed = 7\n",
    "kfold = cross_validation.KFold(n=num_instances, n_folds=num_folds, random_state=seed)\n",
    "model = Lasso()\n",
    "scoring = 'mean_squared_error'\n",
    "results = cross_validation.cross_val_score(model, X, Y, cv=kfold, scoring=scoring)\n",
    "print(results.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ElasticNet Regression\n",
    "\n",
    "- ElasticNet combines the properties of both Ridge Regression and LASSO regression.\n",
    "- Minimizes complexity (magnitude and number of regression coefficients) by penalizing the model using both the l2-norm (sum squared coefficient values) and the l1-norm (sum absolute coefficient values).\n",
    "- Uses the [ElasticNet](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.ElasticNet.html) class.\n",
    "- Result is *mean squared error*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-31.1645737142\n"
     ]
    }
   ],
   "source": [
    "# ElasticNet Regression\n",
    "import pandas\n",
    "from sklearn import cross_validation\n",
    "from sklearn.linear_model import ElasticNet\n",
    "url = \"housing.data\"\n",
    "names = ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT', 'MEDV']\n",
    "dataframe = pandas.read_csv(url, delim_whitespace=True, names=names)\n",
    "array = dataframe.values\n",
    "X = array[:,0:13]\n",
    "Y = array[:,13]\n",
    "num_folds = 10\n",
    "num_instances = len(X)\n",
    "seed = 7\n",
    "kfold = cross_validation.KFold(n=num_instances, n_folds=num_folds, random_state=seed)\n",
    "model = ElasticNet()\n",
    "scoring = 'mean_squared_error'\n",
    "results = cross_validation.cross_val_score(model, X, Y, cv=kfold, scoring=scoring)\n",
    "print(results.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nonlinear Machine Learning Algorithms\n",
    "\n",
    "#### K-Nearest Neighbors [ii]\n",
    "\n",
    "- K-Nearest Neighbors (KNN) finds the most similar instances in the training data and uses the mean distance of the neighbors to calculate a prediction.\n",
    "- By default, distance metric (metric argument) is the [Minkowski distance] (https://en.wikipedia.org/wiki/Minkowski_distance)\n",
    "- Example uses the [KNeighborsRegressor](http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsRegressor.html) class.\n",
    "- Result is *mean squared error*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-107.28683898\n"
     ]
    }
   ],
   "source": [
    "# KNN Regression\n",
    "import pandas\n",
    "from sklearn import cross_validation\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "url = \"housing.data\"\n",
    "names = ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT', 'MEDV']\n",
    "dataframe = pandas.read_csv(url, delim_whitespace=True, names=names)\n",
    "array = dataframe.values\n",
    "X = array[:,0:13]\n",
    "Y = array[:,13]\n",
    "num_folds = 10\n",
    "num_instances = len(X)\n",
    "seed = 7\n",
    "kfold = cross_validation.KFold(n=num_instances, n_folds=num_folds, random_state=seed)\n",
    "model = KNeighborsRegressor()\n",
    "scoring = 'mean_squared_error'\n",
    "results = cross_validation.cross_val_score(model, X, Y, cv=kfold, scoring=scoring)\n",
    "print(results.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classification and Regression Trees\n",
    "\n",
    "- Classification and Regression Trees (CART) use the training data to select the best points to split the data to minimize cost. \n",
    "- Default cost metric for regression decision trees is the mean squared error.\n",
    "- Example uses the [DecisionTreeRegressor](http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html) class.\n",
    "- Result is *mean squared error*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-42.7628411765\n"
     ]
    }
   ],
   "source": [
    "# Decision Tree Regression\n",
    "import pandas\n",
    "from sklearn import cross_validation\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "url = \"housing.data\"\n",
    "names = ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT', 'MEDV']\n",
    "dataframe = pandas.read_csv(url, delim_whitespace=True, names=names)\n",
    "array = dataframe.values\n",
    "X = array[:,0:13]\n",
    "Y = array[:,13]\n",
    "num_folds = 10\n",
    "num_instances = len(X)\n",
    "seed = 7\n",
    "kfold = cross_validation.KFold(n=num_instances, n_folds=num_folds, random_state=seed)\n",
    "model = DecisionTreeRegressor()\n",
    "scoring = 'mean_squared_error'\n",
    "results = cross_validation.cross_val_score(model, X, Y, cv=kfold, scoring=scoring)\n",
    "print(results.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Support Vector Machines\n",
    "\n",
    "- Support Vector Machines (SVM) were developed for binary classification. Has been enhanced to predict real-valued problems called Support Vector Regression (SVR).\n",
    "- Example uses the [SVR](http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVR.html) class.\n",
    "- Result is *mean squared error*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-91.0478243332\n"
     ]
    }
   ],
   "source": [
    "# SVM Regression\n",
    "import pandas\n",
    "from sklearn import cross_validation\n",
    "from sklearn.svm import SVR\n",
    "url = \"housing.data\"\n",
    "names = ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT', 'MEDV']\n",
    "dataframe = pandas.read_csv(url, delim_whitespace=True, names=names)\n",
    "array = dataframe.values\n",
    "X = array[:,0:13]\n",
    "Y = array[:,13]\n",
    "num_folds = 10\n",
    "num_instances = len(X)\n",
    "seed = 7\n",
    "kfold = cross_validation.KFold(n=num_instances, n_folds=num_folds, random_state=seed)\n",
    "model = SVR()\n",
    "scoring = 'mean_squared_error'\n",
    "results = cross_validation.cross_val_score(model, X, Y, cv=kfold, scoring=scoring)\n",
    "print(results.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary of Regression Algorithm Results with Mean Estimated Accuracy\n",
    "\n",
    "- Linear Regression: -34.7052559445\n",
    "- Ridge Regression: -34.0782462093\n",
    "- LASSO Linear Regression: -34.4640845883\n",
    "- Elastic Net Regression: -31.1645737142\n",
    "- **K-Nearest Neighbors: -107.28683898**\n",
    "- Classification and Regression Trees: -42.7628411765\n",
    "- Support Vector Machines: -91.0478243332"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare Machine Learning Algorithms\n",
    "\n",
    "### Classification\n",
    "\n",
    "Test harness is used to be able to consistently compare algorithms on the same dataset. 10-fold cross validation is used to score each algorithm. Same random seed is used for each algorithm, which ensures consistent splits.\n",
    "Algorithms compared are:\n",
    "\n",
    "- Logistic Regression\n",
    "- Linear Discriminant Analysis\n",
    "- K-Nearest Neighbors\n",
    "- Classification and Regression Trees\n",
    "- Naive Bayes\n",
    "- Support Vector Machines\n",
    "\n",
    "This example uses the UCI Pima dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR: 0.769515 (0.048411)\n",
      "LDA: 0.773462 (0.051592)\n",
      "KNN: 0.726555 (0.061821)\n",
      "CART: 0.690055 (0.052844)\n",
      "NB: 0.755178 (0.042766)\n",
      "SVM: 0.651025 (0.072141)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAgkAAAF9CAYAAACOOfuyAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRU5ErkJggg==\n",
      "text/plain": [
       ""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Compare Algorithms\n",
    "%matplotlib inline\n",
    "import pandas\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import cross_validation\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "# load dataset\n",
    "url = \"pima.data\"\n",
    "names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
    "dataframe = pandas.read_csv(url, names=names)\n",
    "array = dataframe.values\n",
    "X = array[:,0:8]\n",
    "Y = array[:,8]\n",
    "# prepare configuration for cross validation test harness\n",
    "num_folds = 10\n",
    "num_instances = len(X)\n",
    "seed = 7\n",
    "# prepare models\n",
    "models = []\n",
    "models.append(('LR', LogisticRegression()))\n",
    "models.append(('LDA', LinearDiscriminantAnalysis()))\n",
    "models.append(('KNN', KNeighborsClassifier()))\n",
    "models.append(('CART', DecisionTreeClassifier()))\n",
    "models.append(('NB', GaussianNB()))\n",
    "models.append(('SVM', SVC()))\n",
    "# evaluate each model in turn\n",
    "results = []\n",
    "names = []\n",
    "scoring = 'accuracy'\n",
    "for name, model in models:\n",
    "    kfold = cross_validation.KFold(n=num_instances, n_folds=num_folds, random_state=seed)\n",
    "    cv_results = cross_validation.cross_val_score(model, X, Y, cv=kfold, scoring=scoring)\n",
    "    results.append(cv_results)\n",
    "    names.append(name)\n",
    "    msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n",
    "    print(msg)\n",
    "# boxplot algorithm comparison\n",
    "fig = plt.figure()\n",
    "fig.suptitle('Algorithm Comparison')\n",
    "ax = fig.add_subplot(111)\n",
    "plt.boxplot(results)\n",
    "ax.set_xticklabels(names)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on these results, **Linear Regression** and **LDA** should be explored further."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save and Load Models\n",
    "\n",
    "### Save Model with pickle\n",
    "\n",
    "Saving and loading a machine learning model is important to make future predictions on unseen data. To save a model to a file, you can use Python's pickle, which serializes the ML algorithms before saving to a file. Loading does the inverse operation.\n",
    "\n",
    "This example trains a logistic regression model on the Pima Indians onset of diabetes dataset, saves the model as **finalized_model.sav** and loads it to make predictions on the unseen test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.761904761905\n"
     ]
    }
   ],
   "source": [
    "# Save Model Using Pickle\n",
    "import pandas\n",
    "from sklearn import cross_validation\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import pickle\n",
    "url = \"pima.data\"\n",
    "names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
    "dataframe = pandas.read_csv(url, names=names)\n",
    "array = dataframe.values\n",
    "X = array[:,0:8]\n",
    "Y = array[:,8]\n",
    "test_size = 0.3\n",
    "seed = 7\n",
    "X_train, X_test, Y_train, Y_test = cross_validation.train_test_split(X, Y, test_size=test_size, random_state=seed)\n",
    "# Fit the model on 33%\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, Y_train)\n",
    "# save the model to disk\n",
    "filename = 'finalized_model.sav'\n",
    "pickle.dump(model, open(filename, 'wb'))\n",
    "\n",
    "# some time later...\n",
    "\n",
    "# load the model from disk\n",
    "loaded_model = pickle.load(open(filename, 'rb'))\n",
    "result = loaded_model.score(X_test, Y_test)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save and Load Model with Joblib\n",
    "\n",
    "You can also use [Joblib](https://pypi.python.org/pypi/joblib) to save and load trained model to make predictions on unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.761904761905\n"
     ]
    }
   ],
   "source": [
    "# Save Model Using joblib\n",
    "import pandas\n",
    "from sklearn import cross_validation\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.externals import joblib\n",
    "url = \"pima.data\"\n",
    "names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
    "dataframe = pandas.read_csv(url, names=names)\n",
    "array = dataframe.values\n",
    "X = array[:,0:8]\n",
    "Y = array[:,8]\n",
    "test_size = 0.3\n",
    "seed = 7\n",
    "X_train, X_test, Y_train, Y_test = cross_validation.train_test_split(X, Y, test_size=test_size, random_state=seed)\n",
    "# Fit the model on 30%\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, Y_train)\n",
    "# save the model to disk\n",
    "filename = 'finalized_model.sav'\n",
    "joblib.dump(model, filename)\n",
    "\n",
    "# some time later...\n",
    "\n",
    "# load the model from disk\n",
    "loaded_model = joblib.load(filename)\n",
    "result = loaded_model.score(X_test, Y_test)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Automate the process using the Pipeline and FeatureUnion utilities to avoid data leakage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline 1: Data Preparation and Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To avoid leaking data from your *training data set* to your *test data set*, its recommended to set up a **test harness**. A test harness should contain tests for data preparation and training/test set splits. \n",
    "\n",
    "1. Standardize the data.\n",
    "1. Learn a Linear Discriminant Analysis model.\n",
    "\n",
    "Below is an example with 10-fold cross validation. A couple of the cells in this step output the description and head for the dataframe, for information purposes. This would *not* be part of the automated test harness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Standardize data and create model\n",
    "from pandas import read_csv\n",
    "from sklearn.cross_validation import KFold\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "# load data\n",
    "url = \"pima.data\"\n",
    "names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
    "dataframe = read_csv(url, names=names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's inspect the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "
\n",
       "\n",
       "  \n",
       "    \n",
       "      \n",
       "      \n",
       "      \n",
       "      \n",
       "      \n",
       "      \n",
       "      \n",
       "      \n",
       "      \n",
       "      \n",
       "    \n",
       "  \n",
       "  \n",
       "    \n",
       "      \n",
       "      \n",
       "      \n",
       "      \n",
       "      \n",
       "      \n",
       "      \n",
       "      \n",
       "      \n",
       "      \n",
       "    \n",
       "    \n",
       "      \n",
       "      \n",
       "      \n",
       "      \n",
       "      \n",
       "      \n",
       "      \n",
       "      \n",
       "      \n",
       "      \n",
       "    \n",
       "    \n",
       "      \n",
       "      \n",
       "      \n",
       "      \n",
       "      \n",
       "      \n",
       "      \n",
       "      \n",
       "      \n",
       "      \n",
       "    \n",
       "    \n",
       "      \n",
       "      \n",
       "      \n",
       "      \n",
       "      \n",
       "      \n",
       "      \n",
       "      \n",
       "      \n",
       "      \n",
       "    \n",
       "    \n",
       "      \n",
       "      \n",
       "      \n",
       "      \n",
       "      \n",
       "      \n",
       "      \n",
       "      \n",
       "      \n",
       "      \n",
       "    \n",
       "    \n",
       "      \n",
       "      \n",
       "      \n",
       "      \n",
       "      \n",
       "      \n",
       "      \n",
       "      \n",
       "      \n",
       "      \n",
       "    \n",
       "    \n",
       "      \n",
       "      \n",
       "      \n",
       "      \n",
       "      \n",
       "      \n",
       "      \n",
       "      \n",
       "      \n",
       "      \n",
       "    \n",
       "    \n",
       "      \n",
       "      \n",
       "      \n",
       "      \n",
       "      \n",
       "      \n",
       "      \n",
       "      \n",
       "      \n",
       "      \n",
       "    \n",
       "  \n",
       "
preg	plas	pres	skin	test	mass	pedi	age	class
count	768.000000	768.000000	768.000000	768.000000	768.000000	768.000000	768.000000	768.000000	768.000000
mean	3.845052	120.894531	69.105469	20.536458	79.799479	31.992578	0.471876	33.240885	0.348958
std	3.369578	31.972618	19.355807	15.952218	115.244002	7.884160	0.331329	11.760232	0.476951
min	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.078000	21.000000	0.000000
25%	1.000000	99.000000	62.000000	0.000000	0.000000	27.300000	0.243750	24.000000	0.000000
50%	3.000000	117.000000	72.000000	23.000000	30.500000	32.000000	0.372500	29.000000	0.000000
75%	6.000000	140.250000	80.000000	32.000000	127.250000	36.600000	0.626250	41.000000	1.000000
max	17.000000	199.000000	122.000000	99.000000	846.000000	67.100000	2.420000	81.000000	1.000000
\n",
       "
"
      ],
      "text/plain": [
       "             preg        plas        pres        skin        test        mass  \\\n",
       "count  768.000000  768.000000  768.000000  768.000000  768.000000  768.000000   \n",
       "mean     3.845052  120.894531   69.105469   20.536458   79.799479   31.992578   \n",
       "std      3.369578   31.972618   19.355807   15.952218  115.244002    7.884160   \n",
       "min      0.000000    0.000000    0.000000    0.000000    0.000000    0.000000   \n",
       "25%      1.000000   99.000000   62.000000    0.000000    0.000000   27.300000   \n",
       "50%      3.000000  117.000000   72.000000   23.000000   30.500000   32.000000   \n",
       "75%      6.000000  140.250000   80.000000   32.000000  127.250000   36.600000   \n",
       "max     17.000000  199.000000  122.000000   99.000000  846.000000   67.100000   \n",
       "\n",
       "             pedi         age       class  \n",
       "count  768.000000  768.000000  768.000000  \n",
       "mean     0.471876   33.240885    0.348958  \n",
       "std      0.331329   11.760232    0.476951  \n",
       "min      0.078000   21.000000    0.000000  \n",
       "25%      0.243750   24.000000    0.000000  \n",
       "50%      0.372500   29.000000    0.000000  \n",
       "75%      0.626250   41.000000    1.000000  \n",
       "max      2.420000   81.000000    1.000000  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Look at df's description\n",
    "dataframe.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "
\n",
       "\n",
       "  \n",
       "    \n",
       "      \n",
       "      \n",
       "      \n",
       "      \n",
       "      \n",
       "      \n",
       "      \n",
       "      \n",
       "      \n",
       "      \n",
       "    \n",
       "  \n",
       "  \n",
       "    \n",
       "      \n",
       "      \n",
       "      \n",
       "      \n",
       "      \n",
       "      \n",
       "      \n",
       "      \n",
       "      \n",
       "      \n",
       "    \n",
       "    \n",
       "      \n",
       "      \n",
       "      \n",
       "      \n",
       "      \n",
       "      \n",
       "      \n",
       "      \n",
       "      \n",
       "      \n",
       "    \n",
       "    \n",
       "      \n",
       "      \n",
       "      \n",
       "      \n",
       "      \n",
       "      \n",
       "      \n",
       "      \n",
       "      \n",
       "      \n",
       "    \n",
       "    \n",
       "      \n",
       "      \n",
       "      \n",
       "      \n",
       "      \n",
       "      \n",
       "      \n",
       "      \n",
       "      \n",
       "      \n",
       "    \n",
       "    \n",
       "      \n",
       "      \n",
       "      \n",
       "      \n",
       "      \n",
       "      \n",
       "      \n",
       "      \n",
       "      \n",
       "      \n",
       "    \n",
       "  \n",
       "
preg	plas	pres	skin	test	mass	pedi	age	class
0	6	148	72	35	0	33.6	0.627	50	1
1	1	85	66	29	0	26.6	0.351	31	0
2	8	183	64	0	0	23.3	0.672	32	1
3	1	89	66	23	94	28.1	0.167	21	0
4	0	137	40	35	168	43.1	2.288	33	1
\n",
       "
"
      ],
      "text/plain": [
       "   preg  plas  pres  skin  test  mass   pedi  age  class\n",
       "0     6   148    72    35     0  33.6  0.627   50      1\n",
       "1     1    85    66    29     0  26.6  0.351   31      0\n",
       "2     8   183    64     0     0  23.3  0.672   32      1\n",
       "3     1    89    66    23    94  28.1  0.167   21      0\n",
       "4     0   137    40    35   168  43.1  2.288   33      1"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Look at first five rows of df, to see what we've got.\n",
    "dataframe.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, lets finish setting up the test harness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.773462064252\n"
     ]
    }
   ],
   "source": [
    "array = dataframe.values\n",
    "X = array[:,0:8]\n",
    "Y = array[:,8]\n",
    "# create pipeline\n",
    "estimators = []\n",
    "estimators.append(('standardize', StandardScaler()))\n",
    "estimators.append(('lda', LinearDiscriminantAnalysis()))\n",
    "model = Pipeline(estimators)\n",
    "# evaluate pipeline\n",
    "num_folds = 10\n",
    "num_instances = len(X)\n",
    "seed = 7\n",
    "kfold = KFold(n=num_instances, n_folds=num_folds, random_state=seed)\n",
    "results = cross_val_score(model, X, Y, cv=kfold)\n",
    "print(results.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline 2: Feature Extraction and Modeling\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data leakage can also occur from *feature extraction*. Feature extraction programs should only consider your training dataset.\n",
    "\n",
    "**FeatureUnion** allows the results of multiple feature selection and extraction procedures to be combined into a larger dataset to train a model. This occurs on each fold of the cross validation procedure.\n",
    "\n",
    "This can be broken down into the following four steps:\n",
    "\n",
    "- Feature Extraction with Principal Component Analysis (3 features)\n",
    "- Feature Extraction with Statistical Selection (6 features)\n",
    "- Feature Union\n",
    "- Learn a Logistic Regression Model\n",
    "\n",
    "Then, evaluate using 10-fold cross validation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.776042378674\n"
     ]
    }
   ],
   "source": [
    "# Create a pipeline that extracts features from the data then creates a model\n",
    "from pandas import read_csv\n",
    "from sklearn.cross_validation import KFold\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "# load data\n",
    "url = \"pima.data\"\n",
    "names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
    "dataframe = read_csv(url, names=names)\n",
    "array = dataframe.values\n",
    "X = array[:,0:8]\n",
    "Y = array[:,8]\n",
    "# create feature union\n",
    "features = []\n",
    "features.append(('pca', PCA(n_components=3)))\n",
    "features.append(('select_best', SelectKBest(k=6)))\n",
    "feature_union = FeatureUnion(features)\n",
    "# create pipeline\n",
    "estimators = []\n",
    "estimators.append(('feature_union', feature_union))\n",
    "estimators.append(('logistic', LogisticRegression()))\n",
    "model = Pipeline(estimators)\n",
    "# evaluate pipeline\n",
    "num_folds = 10\n",
    "num_instances = len(X)\n",
    "seed = 7\n",
    "kfold = KFold(n=num_instances, n_folds=num_folds, random_state=seed)\n",
    "results = cross_val_score(model, X, Y, cv=kfold)\n",
    "print(results.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}